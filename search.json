[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of conduct: Creating a friendly and intellectually stimulating space",
    "section": "",
    "text": "Version 1.0, 2023-08-04 (This CoC is based on the CoC of the German Reproducibility Network, Version 1.0, from 2022-11-09)\nEpistemic responsibility1 – which is based on virtues such as open-mindedness, diligence, and honesty – and interpersonal respect are central values of this project. Therefore we aim to create an open, friendly, diverse, inclusive, and welcoming community and scholarly context in which insights and criticisms are welcome from all participants, regardless of personal characteristics or academic rank.\n\n\n\nEpistemic responsibility and trustworthiness are core values for us as scientists. Critical discussion and evaluation of research has a vital function for the self-correction and progress of science, and we gracefully accept constructive criticism. When criticizing others’ research, we differentiate between the research (as output) and the researcher as a person. We acknowledge that such a separation is sometimes difficult. We are aware of the impact that criticism can have on the lives and careers of researchers, in particular early career researchers, and always aim to balance the values of intellectual honesty and kindness towards our colleagues.\nWe are convinced that a diversity of personal backgrounds and viewpoints is key to scientific progress. We embrace this diversity and believe in the power of collaboration and co-creation. We include as many people as possible in group interactions by being respectful and inviting.\nWe discuss views and claims based on the evidence and the quality of arguments, not based on the status of the people making the claim, nor their personal characteristics or their academic rank.\nScientific debates can feel challenging or even uncomfortable. But also in heated debates we show respect and civility to our fellow colleagues, without discrimination or personal attacks.\nWe respect the private sphere of other people. We never share others’ private information without their consent.\n\n\n\n\n… any form of public or private harassment, either in person, on event platforms or social media. This includes abusive, discriminatory, derogatory, or demeaning language and behaviour. Sexual language and imagery is not appropriate for any event venue or talks. Participants violating these rules may be sanctioned or expelled from the project.\n\n\n\nProject maintainers are responsible for clarifying and enforcing the standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all project spaces, and also applies when an individual is officially representing the project in public spaces. Examples of representing our project include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nIf you think that a project member acts against these guidelines, please report this to one of the project maintainers. The complaint will be investigated based on the principles of impartiality, proportionality, and due process ensuring the anonymity of both reporter and reportee. We ask all involved parties to await the outcome of such investigations before they publicly comment or act on potentially unfounded allegations. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nEcheverri, S. (2011). Epistemic Responsibility and Perceptual Experience. In D. Lauer, C. Laudou, R. Celikates, & G. W. Bertram (Hrsg.), Expérience Et Réflexivité: Perspectives au-Delà de L’Empirisme Et de L’Idéalisme. L’harmattan.\nLechner, I. M., Mokkink, L., de Ridder, J., van Woudenberg, R., Bouter, L., & Tijdink, J. K. (2022). The core epistemic responsibilities of universities: Results from a Delphi study Preprint. Open Science Framework.\n\n\n\n\nCoC of the German Reproducibility Network\nFriendly Space Policy of the Barcamp Open Science\nThe TU Eindhoven Code of Scientific Conduct\nThe IGDORE CoC\nSimine Vazire’s „Oath for Scientists’\nPeels, R., van Woudenberg, R., de Ridder, J., & Bouter, L. (2020). Academia’s Big Five: A normative taxonomy for the epistemic responsibilities of universities [version 2; peer review: 2 approved]. F1000Research, 8(862). https://doi.org/10.12688/f1000research.19459.2\nThe Contributor Covenant, version 2.1"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#this-coc-is-inspired-by",
    "href": "CODE_OF_CONDUCT.html#this-coc-is-inspired-by",
    "title": "Code of conduct: Creating a friendly and intellectually stimulating space",
    "section": "",
    "text": "CoC of the German Reproducibility Network\nFriendly Space Policy of the Barcamp Open Science\nThe TU Eindhoven Code of Scientific Conduct\nThe IGDORE CoC\nSimine Vazire’s „Oath for Scientists’\nPeels, R., van Woudenberg, R., de Ridder, J., & Bouter, L. (2020). Academia’s Big Five: A normative taxonomy for the epistemic responsibilities of universities [version 2; peer review: 2 approved]. F1000Research, 8(862). https://doi.org/10.12688/f1000research.19459.2\nThe Contributor Covenant, version 2.1"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#footnotes",
    "href": "CODE_OF_CONDUCT.html#footnotes",
    "title": "Code of conduct: Creating a friendly and intellectually stimulating space",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEpistemic Responsibility is about the goal to ‘produce, maintain, and disseminate knowledge and other knowledge-related (or: epistemic) goods, such as insight, rational belief, and understanding’ (Lechner et al, 2022) and has been defined as “related to the capacity to engage in adequate policies in the search of truth, the ability to give reasons, or the readiness to revise one’s beliefs in the light of new evidence.’ (Echeverri, 2011).↩︎"
  },
  {
    "objectID": "tech_doc/software_indicators.html",
    "href": "tech_doc/software_indicators.html",
    "title": "Indicators for research software",
    "section": "",
    "text": "To be done.",
    "crumbs": [
      "⎔ Technical documentation",
      "&nbsp;&nbsp;Indicators for Research Software"
    ]
  },
  {
    "objectID": "tech_doc/data_indicators.html",
    "href": "tech_doc/data_indicators.html",
    "title": "Indicators for data sets",
    "section": "",
    "text": "To be done.",
    "crumbs": [
      "⎔ Technical documentation",
      "&nbsp;&nbsp;Indicators for Data Sets"
    ]
  },
  {
    "objectID": "tech_doc/EP-practical_relevance_indicators.html",
    "href": "tech_doc/EP-practical_relevance_indicators.html",
    "title": "Practical relevance",
    "section": "",
    "text": "Notes:\n\nYou can browse all available indicators (also archived versions) on this website.\nAlthough you can click the checkboxes on this preview, this does not have any effect. The checkboxes are only for visualization purposes. The actual app can be found here.",
    "crumbs": [
      "⎔ Technical documentation",
      "&nbsp;&nbsp;Practical Relevance"
    ]
  },
  {
    "objectID": "tech_doc/EP-epistemic_goals_indicators.html",
    "href": "tech_doc/EP-epistemic_goals_indicators.html",
    "title": "Epistemic goals",
    "section": "",
    "text": "Notes:\n\nYou can browse all available indicators (also archived versions) on this website.\nAlthough you can click the checkboxes on this preview, this does not have any effect. The checkboxes are only for visualization purposes. The actual app can be found here.",
    "crumbs": [
      "⎔ Technical documentation",
      "&nbsp;&nbsp;Epistemic Goals"
    ]
  },
  {
    "objectID": "rating_schemes.html",
    "href": "rating_schemes.html",
    "title": "📑 The Rating Schemes",
    "section": "",
    "text": "TODO: Show all indicators for the three ratings schemes\nThe rating sheets are versioned. Indicators from the core set start with the prefix ‘P’ (Publications), ‘D’ (Data) or ‘S’ (Software)."
  },
  {
    "objectID": "publications/Commentaries.html",
    "href": "publications/Commentaries.html",
    "title": "Commentaries by the scientific community",
    "section": "",
    "text": "You can find 15 comments to our target papers 1 and 2 published in Meta-Psychology:\n\nBrown, G. (2024). A broader view of research contributions: Necessary adjustments to DORA for hiring and promotion in psychology. Meta-Psychology, 2024 (8), MP.2022.3652. https://doi.org/10.15626/MP.2024\nSandoval-Lentisco, A. (2022). Commentary: Responsible Research Assessment: Implementing DORA for hiring and promotion in psychology. Meta-Psychology, 2024 (8), MP.2022.3655. https://doi.org/10.15626/MP.2022.3655\nKarhulahti, V. (2023). Interdisciplinary value. Meta-Psychology, 2024 (8), MP.2023.3679. https://doi.org/10.15626/MP.2023.3679\nWitte, E. H. (2023). Comment on: Responsible Research Assessment I and Responsible Research Assessment II. Meta-Psychology, 2024 (8), MP.2023.3685. https://doi.org/10.15626/MP.2023.3685\nStengelin, R., Bohn, M., Sanchez-Amaro, A., Haun, D., Thiele, M., Allritz, M., … Schuhmacher, N. (2023). Responsible Research is also concerned with generalizability: Recognizing efforts to reflect upon and increase generalizability in hiring and promotion decisions in psychology. Meta-Psychology, 2024 (8), MP.2023.3695. https://doi.org/10.15626/MP.2023.3695\nBrandmaier, A. M., Ernst, M. S., & Peikert, A. (2023). Assessing rigor and impact of research software for hiring and promotion in psychology: A comment on Gärtner et al. (2022). **Meta-Psychology, 2024 (8), MP.2023.3715. https://doi.org/10.15626/MP.2023.3715\nFrischkorn, G. T. (2023). Responsible Research Assessment requires structural more than procedural reforms. Meta-Psychology, 2024 (8), MP.2023.3734. https://doi.org/10.15626/MP.2023.3734\nDames, H., Musfeld, P., Popov, V., Oberauer, K., & Frischkorn, G. T. (2023). Responsible Research Assessment Should Prioritize Theory Development and Testing Over Ticking Open Science Boxes. Meta-Psychology, 2024 (8), MP.2023.3735. https://doi.org/10.15626/MP.2023.3735\nSyed, M. (2023). Valuing Preprints Must be Part of Responsible Research Assessment. Meta-Psychology, 2024 (8), MP.2023.3758. https://doi.org/10.15626/MP.2023.3758\nHansen, M., Beitner, J., Horz, H., & Schultze, M. (2023). Indicators for teaching assessment. Meta-Psychology, 2024 (8), MP.2023.3763. https://doi.org/10.15626/MP.2023.3763\nHostler, T. (2023). Research assessment using a narrow definition of “research quality” is an act of gatekeeping: A comment on Gärtner et al. (2022). **Meta-Psychology, 2024 (8), MP.2023.3764. https://doi.org/10.15626/MP.2023.3764\nAuger, V., & Claes, N. (2023). Comment on “Responsible Research Assessment: Implementing DORA for hiring and promotion in psychology”. Meta-Psychology, 2024 (8), MP.2023.3779. https://doi.org/10.15626/MP.2023.3779\nFink-Lamotte, J., Hilbert, K., Bentz, D., Blackwell, S. E., Boehnke, J. R., Burghardt, J., … Niemeyer, H. (2023). Response to responsible research assessment I and II from the perspective of the DGPs working group on open science in clinical psychology. Meta-Psychology, 2024 (8), MP.2023.3794. https://doi.org/10.15626/MP.2023.3794\nBrandt, H., Henninger, M., Ulitzsch, E., Kleinke, K., & Schäfer, T. (2023). Responsible research assessment in the area of methodological or quantitative research: A comment on Gärtner et al. (2022). **Meta-Psychology, 2024 (8), MP.2023.3796. https://doi.org/10.15626/MP.2023.3796\nUlpts, S. (2023). Responsible assessment of what research? Beware of epistemic diversity! Meta-Psychology, 2024 (8), MP.2023.3797. https://doi.org/10.15626/MP.2023.3797\n\nFurthermore, there are 6 additional commentaries (in German) published in the Psychologische Rundschau:\n\nLange, J., Degner, J., Gleibs, I. H., & Jonas, E. (2023). Fachgruppe Sozialpsychologie: Faires und valides Shortlisting in Phase 1. Psychologische Rundschau, 74(3), 187–190. https://doi.org/10.1026/0033-3042/a000641\nNiessen, C., Melchers, K. G., Ohly, S., Fay, D., Handke, L., & Kern, U. M. (2023). Fachgruppe Arbeits-, Organisations- und Wirtschaftspsychologie: Ein Plädoyer für breit gewählte und anforderungsbezogene Leistungsindikatoren. Psychologische Rundschau, 74(3), 180–182. https://doi.org/10.1026/0033-3042/a000637\nOrtner, T., Kretzschmar, A., Rauthmann, J. F., & Tibubos, A. N. (2023). Fachgruppe Differentielle Psychologie, Persönlichkeitspsychologie und psychologische Diagnostik: Berufungsverfahren unter einer diagnostischen Perspektive fundiert durchführen. Psychologische Rundschau, 74(3), 182–184. https://doi.org/10.1026/0033-3042/a000638\nSchwartz, B., Szota, K., Schmitz, J., Lueken, U., & Lincoln, T. (2023). Fachgruppe Klinische Psychologie und Psychotherapie: Mehr Differenzierung nach Fachgebieten. Psychologische Rundschau, 74(3), 184–185. https://doi.org/10.1026/0033-3042/a000639\nSparfeldt, J. R., Spörer, N., Greiff, S., & Schneider, R. (2023). Fachgruppe Pädagogische Psychologie: Ein Plädoyer für valide‍(re) Bewertungen der wissenschaftlichen Leistungen in Berufungsverfahren. Psychologische Rundschau, 74(3), 185–187. https://doi.org/10.1026/0033-3042/a000640\nStroebe, W., & Strack, F. (2023). Kommentare zu Gärtner, A. et al. (2023). Empfehlungen zur Bewertung wissenschaftlicher Leistungen bei Berufungsverfahren in der Psychologie: Zweierlei Maß? Warum manche Psychologen den Gebrauch von quantitativen Indikatoren der Forschungsqualität ablehnen. Psychologische Rundschau, 74(3), 175–179. https://doi.org/10.1026/0033-3042/a000631",
    "crumbs": [
      "📔 Publications",
      "💬 Commentaries"
    ]
  },
  {
    "objectID": "news/2024-03-21-Einstein_Foundation_Award.html",
    "href": "news/2024-03-21-Einstein_Foundation_Award.html",
    "title": "Einstein Foundation Award",
    "section": "",
    "text": "Anne Gärtner received the Einstein Foundation Early Career Award 2023 for the project on Responsible Research Assessment. The project is based on RESQUE and aims to further develop and evaluate the criteria.\nThe video of the award ceremony can be seen here.\nWatch the project presentation here.\n\n\n\nEinstein Foundation Award\n\n\n\n\n\nAward Ceremony at Bode Museum, Berlin"
  },
  {
    "objectID": "news/2023-12-12-website.html",
    "href": "news/2023-12-12-website.html",
    "title": "RESQUE website is live!",
    "section": "",
    "text": "We launched the RESQUE website. All material will be collected here, including news and information about case studies, evaluation projects, new indicator packs, etc."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "News",
    "section": "",
    "text": "Einstein Foundation Award\n\n\n\n\n\n\n\n\n\n\n\n2024-03-21\n\n\n\n\n\n\n\n\n\n\n\n\nNew indicators for theory-guided research developed\n\n\n\n\n\n\n\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\nRESQUE website is live!\n\n\n\n\n\n\n\n\n\n\n\n2023-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nPreprint published by Franka Etzel on an evaluation study\n\n\n\n\n\n\n\n\n\n\n\n2023-09-04\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "get_profile.html",
    "href": "get_profile.html",
    "title": "👩‍🔬 For applicants",
    "section": "",
    "text": "Warning: App still under development\n\n\n\n\n\nThe Collector App for entering the data, the R package for profile creation, and the Profile Builder website where end-users can retrieve their profile are in beta stage and not finalized yet. Things might change substantially in the near future. If you want to use the RESQUE framework in practice please contact us.\n\n\n\n\nStep 1: Enter the data for up to 10 publications in the RESQUE Collector App app. When done, save the data as a local json file (“Save to file …” in the top left corner).\nStep 2: Go to the RESQUE Profiler app, upload your json file, and get your Research Profile as html file.\n\n\n\n\n\n\n\nNote\n\n\n\nIf a hiring committee requested that you include a RESQUE Profile in your application, please send the json file (see Step 1) to the committee.\n\n\n\nHere is a full demo profile."
  },
  {
    "objectID": "eval_projects.html",
    "href": "eval_projects.html",
    "title": "🔎 Evaluation Projects",
    "section": "",
    "text": "Several research projects work(ed) on the evaluation of the RESQUE rating scheme:\n📔 Etzel, F. T., Seyffert-Müller, A., Schönbrodt, F. D., Kreuzer, L., Gärtner, A., Knischewski, P., & Leising, D. (2024, May 8). Inter-Rater Reliability in Assessing the Methodological Quality of Research Papers in Psychology. https://doi.org/10.31234/osf.io/4w7rb.\n\n\n\n\n\n\nAbstract\n\n\n\nGiven the apparent validity deficiencies of many well-established metrics of research productivity (such as h-indices and journal impact factors), the demand for viable alternatives is growing. This paper presents two empirical studies in which groups of raters (n1 = 3, n2 = 9) assessed the methodological rigor of research papers (k1 = 52, k2 = 110) using detailed catalogs of relatively well-defined quality criteria. The main endpoint in both studies was inter-rater reliability, which is a necessary prerequisite for any subsequent use of such assessments (e.g., as part of hiring or promotion procedures). Both studies showed that the application of several open science practices (e.g., open data, preregistration) may in fact be assessed with good reliability (Kappa &gt; .60, ICCs &gt; .75), even by raters who received little to no training, and within reasonable amounts of time (M1 = 21.7, M2 = 40.2 minutes on average). When aggregated across indicators, inter-rater reliability for this type of assessment was good to excellent (Study 1: ICC(1, 1) = .91, Study 2: ICC(1,1) = .74). A subsample of papers in Study 2 was drawn randomly from the recent literature (2020-2022) and showed that typical papers in contemporary psychology still exhibit very low methodological rigor. Study 2 also showed that criteria related to consensus-building do not yet espouse sufficient reliability. Standard criterion sets for assessing the methodological rigor of empirical research should be used more widely in evaluating submissions to scientific journals, as well as published research (e.g., in evaluating the research productivity of individuals, groups or institutions). Such evaluations will be also facilitated by establishing clearer and more widely-adopted reporting standards.\n\n\nOne Key Finding is that the overall Relative Rigor Score of RESQUE can assessed with good to excellent inter-rater reliability (ICC(1,1) = .91) by student assistants.\n📔 Christoph Heller & Jakob Fink-Lamotte (in prep.) are testing the RESQUE v0.3 rating scheme in the field of clinical psychology."
  },
  {
    "objectID": "for_committees.html",
    "href": "for_committees.html",
    "title": "📊 For hiring/tenure committees",
    "section": "",
    "text": "Warning\n\n\n\nTODO - this is not complete."
  },
  {
    "objectID": "for_committees.html#choose-your-indicators",
    "href": "for_committees.html#choose-your-indicators",
    "title": "📊 For hiring/tenure committees",
    "section": "Choose your indicators",
    "text": "Choose your indicators\nThe RESQUE framework decisively promotes a “buffett table approach”: From the list of available indicators, hiring committees choose those which show a fit to the position and to the preferences and the research culture of the institution.\nWe provide a lean core set with the basic quality indicators that fit to the vast majority of research in any psycghological sub-discipline. This core set is recommended for all hiring committees (although any indicator can be deselected).\nThis can be expanded by multiple expansion packs that contain additional indicators. These packs are tailored to specific research areas or research cultures.\nCurrently, the selection of indicators itself (if you do not want to use the default core set) is done by adjusting the json files in the subfolder /packs of the collector app. In these config files, all indicators can be adjusted, removed, or new indicators added. We suggest that you fork our repository and make your changes in your personal copy. Currently, adapting the questions of the collector app requires some technical knowledge. However, we are happy to assist in customizing the app for your hiring committee."
  },
  {
    "objectID": "for_committees.html#choose-your-weights",
    "href": "for_committees.html#choose-your-weights",
    "title": "📊 For hiring/tenure committees",
    "section": "Choose your weights",
    "text": "Choose your weights\nWe make suggestions for weights that define how much each indicator contributes to the rigor score(s). But every committee can define their own weights.\nIf an indicator gets the weight 0 (but is not removed from the collector app), it becomes a descriptive indicator. It does not contribute to any quality score, but the committee still can see the answers.\n\n\n\n\n\n\n\nTODOs\n\n\n\n\n\n\nShow recommended paragraph in job ad\ncf. Lange et al.: Commit to weights and indicator sets when publishing the job ad; we recommend to publish the weights and indicators along with the ad.\nTemplate for instructions for applicants"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RESQUE: A Research Quality Evaluation scheme for psychological research",
    "section": "",
    "text": "See all news here.\n\n\n\n\n\n2024-03-21: Einstein Foundation Award \n\n\n\n2024-02-26: New indicators for theory-guided research developed \n\n\n\n2023-12-12: RESQUE website is live! \n\n\n\n2023-09-04: Preprint published by Franka Etzel on an evaluation study \n\n\n\n\nNo matching items\nThe Research Quality Evaluation (RESQUE) framework provides recommendations for a responsible research assessment that does not rely on flawed metrics such as the journal impact factor or the h-index.\nIn alignment with the principles of CoARA, this approach acknowledges diverse academic contributions, prioritizes the quality of work rather than its volume, and integrates qualitative peer assessment with the responsible use of quantitative indicators. All proposed indicators are open-source and reproducible."
  },
  {
    "objectID": "index.html#what-is-resque-three-modular-elements",
    "href": "index.html#what-is-resque-three-modular-elements",
    "title": "RESQUE: A Research Quality Evaluation scheme for psychological research",
    "section": "What is RESQUE? Three modular elements",
    "text": "What is RESQUE? Three modular elements\n\n  \n\n    \n      \n        1. Evaluated Indicators\n        \n        ✅ A set of evaluated indicators for aspects of research quality.\n        👉🏼 \"Buffett table\" approach: Escape h-index and JIF by choosing valid indicators that suit your research assessment scenario.\n      \n    \n\n    \n      \n        2. Process Suggestion\n        \n        ✅ A suggestion for a two-stage process and weights for indicators.\n        👉🏼 Change the importance of indicators depending on your use case.\n      \n    \n    \n    \n      \n        3. Tools Set\n        \n        ✅ A set of tools that can be used (optionally):\n        \n          A Collector App where applicants can enter their data\n          Profile creation for CVs\n          Interactive comparative overview for committees\n        \n        👉🏼 Open source, free to use, privacy aware. Of course you can also include the selected indicators in your existing recruitment system.\n      \n    \n  \n\nPrimarily designed to assist hiring and tenure committees, it emphasizes that the indicators and algorithmic methods serve as tools to support, not replace, decision-making processes. By automating the generation of relevant candidate information, this approach enhances the effectiveness of human expertise in evaluating potential hires and tenure candidates.\nRESQUE aims to provide objective quality and impact indicators for three types of research outputs:\n\npublications of empirical studies,\npublished data sets and\nresearch software.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe RESQUE framework is currently in development and the indicators are subject to change. Currently, only indicators for publications are available in a beta version.\n\n\nThe assessment scheme is primarily developed for the field of psychology, but might be easily transferred to neighbouring empirical scientific fields."
  },
  {
    "objectID": "index.html#built-by-the-community-for-the-community",
    "href": "index.html#built-by-the-community-for-the-community",
    "title": "RESQUE: A Research Quality Evaluation scheme for psychological research",
    "section": "Built by the community for the community",
    "text": "Built by the community for the community\nThe SCOPE Framework for Research Evaluation proposes the principle “Evaluate with the evaluated”, advocating for a co-creation process within the communities that are being evaluated. RESQUE is developed in that spirit. We started with four target papers that elicited 40 publihsed commentaries, which led to a major revision. (Many critical commenters now are in the developer team.) We had workshops with most field-specific groups within the DGPs, which also led to substantial revisions and the development of discipline-specific expansion packs."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "RESQUE: A Research Quality Evaluation scheme for psychological research",
    "section": "Resources",
    "text": "Resources\n\n☑️ Rating schemes: Core set\nWith the goal to make data entry for applicants and hiring committees as easy and frictionless as possible, we developed the RESQUE Collector App that allows to provide information for your best research outputs and to export the data in a JSON format.\nWe plan to provide three rating schemes for …\n\nRESQUE-Pubs (Publications, preprints, book chapters, etc.): ✅ v0.4.0, ready for use (but still likely to change).\nRESQUE-Data (Data sets): ❌ under development, not implemented in Collector App yet\nRESQUE-Software (Research software): ❌ under development, not implemented in Collector App yet\n\n\n\n☑︎ Rating schemes: Expansion packs\nWe aim to develop and collect disciplinary expansion packs (EPs) with specific indicators that are relevant for a subfield. If you plan to provide an expansion pack, please get in contact with us (we will help to implement that in the web form).\nSuch EPs ideally are contributed from a legit academic community and stem from a consensus process. Before including EPs in this project, we do a minimal review for suitability.\nThe subdivisions of the German Psychological Society have been asked to discuss the need for discipline-specific indicators.\nAvailable Expansion Packs:\n\nEpistemic Goals\nMulti-/Interdisciplinarity\nPractical Relevance\n\n\n\n📊 The RESQUE Research Profile\nWe provide R scripts that …\n\nload and aggregate multiple json files with RESQUE ratings\nenrich the data with other sources of information (e.g. citation counts & normalized citation counts from OpenAlex, impact metrics from BIP!)\nprovide candidate profiles that can be requested by hiring and tenure committees, or used by candidates as an attachment to their CV or website.\n\nAn interactive dashboard allows to compare multiple candidates and to zoom into the profiles of specific candidates.\nFor more information, see here."
  },
  {
    "objectID": "index.html#partners-and-supporters-of-resque",
    "href": "index.html#partners-and-supporters-of-resque",
    "title": "RESQUE: A Research Quality Evaluation scheme for psychological research",
    "section": "Partners and supporters of RESQUE",
    "text": "Partners and supporters of RESQUE"
  },
  {
    "objectID": "news/2023-09-04-Etzel_preprint.html",
    "href": "news/2023-09-04-Etzel_preprint.html",
    "title": "Preprint published by Franka Etzel on an evaluation study",
    "section": "",
    "text": "63 papers nominated by 21 participating researchers from personality and social psychology were rated by external raters (i.e., not the authors themselves) with the RESQUE-Pubs scheme. Inter-rater reliability, associations between the new and traditional indicators, and feedback from the participants on the new tool were examined. Inter-rater reliability for the three raters varied between the different items of the scheme. Besides a negative association between the new indicators and the uncorrected h-index, no other significant associations were found. The feedback from participants revealed the importance of transparency concerning the scheme.\nThe RESQUE-Pubs scheme good be properly apply applied to 84% of all submitted papers. The average rigor score was around 35%.\n(Note that the RESQUE-Pubs scheme evolved since that study).\nYou can read the study here:\nEtzel, F. T. (2023, September 4). *One step closer towards responsible research assessment in psychology: Evaluation and testing of a new tool in practice. https://doi.org/10.31234/osf.io/3uf7w"
  },
  {
    "objectID": "news/2024-02-26-theory_indicators.html",
    "href": "news/2024-02-26-theory_indicators.html",
    "title": "New indicators for theory-guided research developed",
    "section": "",
    "text": "A working group consisting of Nele Freyer, Jens Lange, Daniel Leising, Philipp Musfeld, and Felix Schönbrodt developed a set of indicators that allows to evaluate theory-guided research.\nThe indicators cover both narrative and formal theories, and they cover both theory development and theory testing.\nIn the general philosophy of RESQUE, these indicators mostly focus on “hygiene factors”: Properties of theory-guided research that are necessary but not sufficient for high-quality research.\nHere are the indicators:\n\n\n\n\nTheory usage and development\n\nIn the following, the term ‘theory’ is used for a set of (interconnected) ideas that explain empirical phenomena. Theories may be expressed in a narrative and/or a more formalized manner. The term ‘model’ is reserved for a (part of a) theory that is expressed mathematically or algorithmically.\n\n\n\n\n\n[P_Theorizing] Theory-guided research and theory development\n\n\n[ Condition: $P_Suitable === ‘Yes’ ]\n\n\n[DescriptiveExploratory] This research was mainly descriptive and/or exploratory, without clear reference to a theory[TheoryGuided] In this research, at least one theory was (further) developed and/or evaluated[NotApplicable] Not applicable / cannot answer the question\n\n\n\n\n\n[P_Theorizing_NAExplanation] ⤷ Not Applicable: Explanation\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘NotApplicable’ ]\n\n\n\n\n\n\n[P_Theorizing_Quality] The paper …\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ ]\n\n\n[P_Theorizing_Quality_NarrativeAccount] contains an explicit narrative account of the phenomena that are to be explained, and of one or more hypothetical mechanisms accounting for them (i.e., a theory).[P_Theorizing_Quality_NarrativeDefinitions] contains explicit narrative definitions of each element (concepts, relationships, etc.) in the theory. All of this is made easily findable (e.g., under a standard heading ‘Definitions’).[P_Theorizing_Quality_FormalizedAccount] contains a formalized account of the theory (i.e., a model) using a commonly accepted standard notation (e.g., mathematical or logical operations, ODD protocol for ABMs). This includes all connections among the elements of the theory, and a formalization of the phenomena that are to be explained.[P_Theorizing_Quality_MathProps] contains a formalized account of the theory (i.e., a model) and explicates the basic mathematical properties of all elements of the model (e.g., scalar vs. matrix; dimensionality; units carrying the information).[P_Theorizing_Quality_PossibleValues] contains a formalized account of the theory (i.e., a model) and explicates the possible values for all elements of the model (i.e., upper and lower bounds, scale level).[P_Theorizing_Quality_ObjectiveDerivation] contains a formalized account of the theory (i.e., a model) and explicitly derives predictions from that. The derivation is objective, in the sense that any person would come to the exact same predictions.[P_Theorizing_Quality_Falsifiability] contains a formalized account of the theory (i.e., a model) and all stated predictions are in principle falsifiable (before considering operationalizations).\n\n\n\n\n\n[P_Theorizing_Type] In what language or system is the theory formulated?\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ ]\n\n\n[Narrative] Narrative[Mathematical] Mathematical[FormalLogic] Formal-Logic[ABM] Agent-based model[Other] Other (please specify)\n\n\n\n\n\n[P_Theory_Type_Other] Theory type/ Language: Other\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ && $P_Theorizing_Type === ‘Other’ ]\n\n\n\n\n\n\n[P_Theorizing_Contribution] How much did you personally contribute to this version of the theory?\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ ]\n\n\n[No] I did not contribute to this version of the theory itself (i.e., I did no theoretical work beyond testing a theory)[Yes] I did contribute significantly to this version of the theory\n\n\n\n\n\n[P_Theorizing_ContributionType] I significantly contributed to …\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ && $P_Theorizing_Contribution === ‘Yes’ ]\n\n\n[P_Theorizing_ContributionType_NarrativeDevelopment] the overall development of an entire narrative theory[P_Theorizing_ContributionType_NarrativeRefinement] the expansion or refinement of an existing narrative theory[P_Theorizing_ContributionType_FormalizationFirst] the first formalization of an existing narrative theory[P_Theorizing_ContributionType_FormalizationStandAlone] the overall development of an entire formal model of which there was no narrative version[P_Theorizing_ContributionType_FormalizationRefinement] the expansion or refinement of an existing formal model\n\n\n\n\n\n[P_Theorizing_Test] Did you test (a part of) this theory?\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ ]\n\n\n[No] No[Yes] Yes\n\n\n\n\n\n[P_Theorizing_Test_Quality] The paper …\n\n\n[ Condition: $P_Suitable === ‘Yes’ && $P_Theorizing === ‘TheoryGuided’ && $P_Theorizing_Test === ‘Yes’ ]\n\n\n[P_Theorizing_Test_Quality_Assumptions] describes all necessary assumptions that were necessary to make the theory testable (including ad-hoc assumptions).[P_Theorizing_Test_Quality_Operationalization] describes which, how, and why elements of the theory are measured, estimated, or fixed to specific values.[P_Theorizing_Test_Quality_Derivation] makes clear how the tested hypotheses logically and stringently follow from the theory.[P_Theorizing_Test_Quality_Evaluation] evaluates the relative and/or absolute model fit with appropriate procedures (e.g., out-of-sample cross-validation, AIC, BIC, LOO, Bayes Factors).[P_Theorizing_Test_Quality_Discussion] contains an explicit discussion as to what the results mean for the theory that was tested."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "The general framework of RESQUE is described in:\n\n[1]  Schönbrodt, F. D., Gärtner, A., Frank, M., Gollwitzer, M., Ihle, M., Mischkowski, D., Phan, L. V., Schmitt, M., Scheel, A. M., Schubert, A.-L., Steinberg, U., & Leising, D. (2022). Responsible Research Assessment I: Implementing DORA for hiring and promotion in psychology. https://doi.org/10.31234/osf.io/rgh5b\n\n\nThe specific RESQUE rating schemes are described in:\n\n[2]  Gärtner, A., Leising, D., & Schönbrodt, F. D. (2022). Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology. https://doi.org/10.31234/osf.io/5yexm\n\nand\n\n[3]  Gärtner, A., Leising, D., & Schönbrodt, F. D. (2023). Empfehlungen zur Bewertung wissenschaftlicher Leistungen bei Berufungsverfahren in der Psychologie. Psychologische Rundschau, 74(3), 166–174. https://doi.org/10.1026/0033-3042/a000630\n\n(Note: these publications refer to version 0.1 of the RESQUE scheme; as the rating scheme will be continuously updated based on community feedback and ongoing evaluation studies, some divergences to these publications will arise).",
    "crumbs": [
      "📔 Publications"
    ]
  },
  {
    "objectID": "publications/Presentations.html",
    "href": "publications/Presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "To get an overview about the RESQUE project, you can look at our presentation slides. In general, the information on this website is more up to date than the slides.\n\nRESQUE: Overview in 8 slides\n\nSlightly longer slide decks with more information:\n\nSchönbrodt (2023-03): Responsible Research Assessment: A practical recommendation for the evaluation of research quality beyond h-index and journal impact factors\nGärtner (2023-09): Responsible Research Assessment II: The RESQUE web form for hiring and promotion\nSlides for meetings for an Expansion Pack\nSchönbrodt (2024-06): Responsible Research Assessment: A pitch for hiring committees",
    "crumbs": [
      "📔 Publications",
      "📊 Presentations"
    ]
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "👥 Team / Contribute",
    "section": "",
    "text": "The RESQUE framework is a collaborative project initiated by the German Psychological Society (DGPs).\nThis project is a community effort with many contributors:\n… all who wrote comments with many constructive suggestions … and many more who gave extended feedback on the proposal! 🙏"
  },
  {
    "objectID": "team.html#how-to-contribute",
    "href": "team.html#how-to-contribute",
    "title": "👥 Team / Contribute",
    "section": "How to contribute",
    "text": "How to contribute\nJoin our mailing list to receive news and updates about the RESQUE framework.\nFor minor updates and minor feature requests, please use the Discussions of the Github repository.\nIf you want to collaborate scientifically on the project, please contact one of the project maintainers (Felix Schönbrodt; Anne Gärtner; Daniel Leising).\nIf you want to develop an expansion pack for your field, please contact Anne Gärtner.\nPlease note and respect our Code of Conduct."
  },
  {
    "objectID": "team.html#partners-and-supporters-of-resque",
    "href": "team.html#partners-and-supporters-of-resque",
    "title": "👥 Team / Contribute",
    "section": "Partners and supporters of RESQUE",
    "text": "Partners and supporters of RESQUE"
  },
  {
    "objectID": "tech_doc/EP-multidisciplinary_indicators.html",
    "href": "tech_doc/EP-multidisciplinary_indicators.html",
    "title": "Multi-/Interdisciplinarity",
    "section": "",
    "text": "Notes:\n\nYou can browse all available indicators (also archived versions) on this website.\nAlthough you can click the checkboxes on this preview, this does not have any effect. The checkboxes are only for visualization purposes. The actual app can be found here.",
    "crumbs": [
      "⎔ Technical documentation",
      "&nbsp;&nbsp;Multi-/Interdisciplinarity"
    ]
  },
  {
    "objectID": "tech_doc/RRS.html",
    "href": "tech_doc/RRS.html",
    "title": "Computation of Relative Rigor Score (RRS)",
    "section": "",
    "text": "There are many ways to conduct science, and RESQUE does not give a precedence to any approach. For example, it does not favor confirmatory research over exploratory research. But whatever approach you choose, it should be done well. So we created quality indicators for different aspects.\nImportantly, you should not be penalized if certain points cannot be achieved in principle. Therefore, we compute the Relative Rigor Scores as a POMP (Percentage of Maximum Points): If an indicator is justifiably ‘not applicable’, the indicator is taken out of the set, the maximum score is adjusted accordingly, allowing for the possibility of achieving 100% of the applicable points.",
    "crumbs": [
      "⎔ Technical documentation",
      "Computation of Relative Rigor Score (RRS)"
    ]
  },
  {
    "objectID": "tech_doc/RRS.html#basic-principles",
    "href": "tech_doc/RRS.html#basic-principles",
    "title": "Computation of Relative Rigor Score (RRS)",
    "section": "",
    "text": "There are many ways to conduct science, and RESQUE does not give a precedence to any approach. For example, it does not favor confirmatory research over exploratory research. But whatever approach you choose, it should be done well. So we created quality indicators for different aspects.\nImportantly, you should not be penalized if certain points cannot be achieved in principle. Therefore, we compute the Relative Rigor Scores as a POMP (Percentage of Maximum Points): If an indicator is justifiably ‘not applicable’, the indicator is taken out of the set, the maximum score is adjusted accordingly, allowing for the possibility of achieving 100% of the applicable points.",
    "crumbs": [
      "⎔ Technical documentation",
      "Computation of Relative Rigor Score (RRS)"
    ]
  },
  {
    "objectID": "tech_doc/RRS.html#equal-weighting-of-research-outputs",
    "href": "tech_doc/RRS.html#equal-weighting-of-research-outputs",
    "title": "Computation of Relative Rigor Score (RRS)",
    "section": "Equal weighting of research outputs",
    "text": "Equal weighting of research outputs\nFor the computation of the overall RRS score, all submitted research outputs are weighted equally. That means, if two publications are available, each publication contributes 50% to the overall score, even if they differ in their maximally attainable points. The reasoning is that it should not be punished if some points cannot be obtained in principle. With our POMP (“percentage of maximum points”) computation, it is ensured that the RRS for each research output can reach 100% even if some indicators are not applicable. But if research outputs with less attainable points contribute less to the overall score, this would be a disadvantage. Therefore each output is weighted equally.",
    "crumbs": [
      "⎔ Technical documentation",
      "Computation of Relative Rigor Score (RRS)"
    ]
  },
  {
    "objectID": "tech_doc/pub_indicators.html",
    "href": "tech_doc/pub_indicators.html",
    "title": "Indicators for publications",
    "section": "",
    "text": "Notes:\n\nYou can browse all available indicators (also archived versions) on this website.\nAlthough you can click the checkboxes on this preview, this does not have any effect. The checkboxes are only for visualization purposes. The actual app can be found here.\nIn the Collector App, many indicators are shown conditionally, depending on answers to previous items. Hence, the actual number of indicators presented to applicants typically is lower.",
    "crumbs": [
      "⎔ Technical documentation",
      "&nbsp;&nbsp;Indicators for Publications"
    ]
  },
  {
    "objectID": "technical_docs.html",
    "href": "technical_docs.html",
    "title": "⎔ Technical documentation",
    "section": "",
    "text": "When you use the RESQUE tools, please cite it as:\n\nSchönbrodt, F., Gärtner, A., & Leising, D. (2024). The RESQUE Framework (Version 0.3) [Computer software]. https://github.com/RESQUE-Framework\n\nYou can also get the current reference with the .cff file in the repository:\n\n\n\nImage CC-BY 4.0 by The Turing Way",
    "crumbs": [
      "⎔ Technical documentation",
      "General information"
    ]
  },
  {
    "objectID": "technical_docs.html#how-to-cite-resque",
    "href": "technical_docs.html#how-to-cite-resque",
    "title": "⎔ Technical documentation",
    "section": "",
    "text": "When you use the RESQUE tools, please cite it as:\n\nSchönbrodt, F., Gärtner, A., & Leising, D. (2024). The RESQUE Framework (Version 0.3) [Computer software]. https://github.com/RESQUE-Framework\n\nYou can also get the current reference with the .cff file in the repository:\n\n\n\nImage CC-BY 4.0 by The Turing Way",
    "crumbs": [
      "⎔ Technical documentation",
      "General information"
    ]
  },
  {
    "objectID": "technical_docs.html#postprocessing-the-applicants-data",
    "href": "technical_docs.html#postprocessing-the-applicants-data",
    "title": "⎔ Technical documentation",
    "section": "Postprocessing the applicants’ data",
    "text": "Postprocessing the applicants’ data\nThe applicants’ data (acquired by the Collector app) is stored in json files that can be imported in R with the RESQUER package. The package preprocesses the data and enriches it with:\n\ncitation metrics from OpenAlex\nimpact metrics from BIP!\nthe TOP factor for the journals the author is publishing in\n\nFurthermore, the package provides an interactive dashboard that allows to compare multiple candidates.",
    "crumbs": [
      "⎔ Technical documentation",
      "General information"
    ]
  }
]